{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMOPiQd/TJnsBuLNJR6XhoJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb0b0e2def3e4ce6a70f83183ad22caf": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_7463dd51c6824946a607ec3f6524b266",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "7463dd51c6824946a607ec3f6524b266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simeonbetapudi/DeepLearningAIEthics/blob/main/LeaderboardCompetition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGWCNTLQ17WE",
        "outputId": "bc67d931-c3da-4f86-e3ce-650fe0ebd812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: bigframes 2.23.0\n",
            "Uninstalling bigframes-2.23.0:\n",
            "  Successfully uninstalled bigframes-2.23.0\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m828.5/828.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Uninstall Colab's bigframes because it conflicts with other installs\n",
        "%pip uninstall -y bigframes\n",
        "# Install Lightning, also let's use \"rich\" progress bars\n",
        "%pip install -Uqq lightning wandb rich einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim, utils\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor, RandomAffine, RandomErasing\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "# Not advocating Lightning over raw pytorch, but it offers some useful abstractions\n",
        "import lightning as L\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "from lightning.pytorch.callbacks import RichProgressBar\n",
        "import wandb\n",
        "import numpy as np\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "s944-VoW2tgt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional: define additional data augmentation transformers for the dataloader\n",
        "train_transforms = torchvision.transforms.Compose([\n",
        "    ToTensor(),\n",
        "    # uncomment next lines for extra augmentations\n",
        "    #RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
        "    #RandomErasing(p=0.2, scale=(0.02, 0.1))\n",
        "])\n",
        "\n",
        "train_ds = MNIST(root='./data', train=True,  download=True, transform=train_transforms)\n",
        "test_ds  = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
        "val_ds = test_ds  #alias val for test\n",
        "print(f\"Data set lengths: train: {len(train_ds)}, test: {len(test_ds)}\")\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128   # could make this bigger; note for MNIST on Colab we're disk-speed limited, not GPU-limited\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=2, shuffle=True, persistent_workers=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=batch_size, num_workers=2, shuffle=False, persistent_workers=True)\n",
        "val_dl = test_dl # alias val <--> test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYaC1K3c2y0y",
        "outputId": "3d0e3ba8-d19e-4868-fc7c-9278855292ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:00<00:00, 20.5MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 486kB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 4.65MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 10.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data set lengths: train: 60000, test: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Code for `show_xs` visualization tool\n",
        "def show_xs(xs, show_stats=True):\n",
        "    \"\"\"A little utility to show one or more images\"\"\"\n",
        "    if type(xs) is not list: xs = list(xs)\n",
        "    ncols = len(xs)\n",
        "    fig, axs = plt.subplots(figsize=(3*ncols,2), ncols=ncols, squeeze=False)\n",
        "    ax = axs.ravel()\n",
        "    for col, x in enumerate(xs):\n",
        "        if len(x.shape)>2: x = x[0] # remove any batch dimension\n",
        "        if show_stats:\n",
        "            if ncols > 1: print(f\"col {col}: \",end=\"\")\n",
        "            print(f\"x.shape = {tuple(x.shape)}, min(x) = {torch.min(x)}, max(x) = {torch.max(x)}\")\n",
        "        digit = ax[col].imshow(x.detach().cpu().numpy(), cmap='gray')\n",
        "        fig.colorbar(digit, ax=ax[col])\n",
        "    plt.show()\n",
        "\n",
        "x, y = next(iter(train_dl))   # pick a few elements from the dataset\n",
        "print(f\"y (target) = {y[:4]} \")\n",
        "\n",
        "show_xs(x[:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "cellView": "form",
        "id": "jouzpUh923K_",
        "outputId": "291e5922-29b6-4ae8-a7c2-de28b45de27b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y (target) = tensor([9, 4, 1, 4]) \n",
            "col 0: x.shape = (28, 28), min(x) = 0.0, max(x) = 1.0\n",
            "col 1: x.shape = (28, 28), min(x) = 0.0, max(x) = 1.0\n",
            "col 2: x.shape = (28, 28), min(x) = 0.0, max(x) = 1.0\n",
            "col 3: x.shape = (28, 28), min(x) = 0.0, max(x) = 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x200 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title `test_inference` visualization code\n",
        "@torch.no_grad()\n",
        "def test_inference(model, idx=None, return_fig=False):\n",
        "    import inspect\n",
        "    model.eval()\n",
        "    if idx is None: idx = torch.randint(len(test_ds), (1,))[0]\n",
        "    if isinstance(idx, int): idx = [idx]\n",
        "    elif isinstance(idx, range): idx = list(idx)\n",
        "    x_batch = torch.stack([test_ds[i][0] for i in idx]).to(model.device)  # images\n",
        "    y_batch = torch.tensor([test_ds[i][1] for i in idx]).to(model.device) # labels\n",
        "    if not model.use_conv: x_batch = x_batch.view(x_batch.size(0), -1)\n",
        "    if 1==len(inspect.signature(model.forward).parameters): # for ae or vae\n",
        "        result = model.forward(x_batch)\n",
        "    else:                                                   # c-vae (later in lesson)\n",
        "        cond = F.one_hot(y_batch, num_classes=10).float()\n",
        "        result = model.forward(x_batch, cond)\n",
        "    z, recon = result[:2]\n",
        "    recon = recon.view(len(idx), 28, 28)\n",
        "    fig, axs = plt.subplots(2, len(idx), figsize=(3*len(idx), 4))\n",
        "    if len(idx) == 1: axs = axs.reshape(2, 1)\n",
        "    for i in range(len(idx)):\n",
        "        axs[0,i].imshow(x_batch[i].view(28,28).cpu(), cmap='gray')\n",
        "        axs[1,i].imshow(recon[i].cpu(), cmap='gray')\n",
        "        if i == 0:\n",
        "            axs[0,0].set_ylabel('Input', fontsize=12)\n",
        "            axs[1,0].set_ylabel('Reconstruction', fontsize=12)\n",
        "    model.train()\n",
        "    if return_fig: return fig\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zd3h3zHW6NWR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(L.LightningModule):\n",
        "    \"\"\"Double convolution block used in U-Net\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(L.LightningModule):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_channels, out_channels))\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(L.LightningModule):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, 2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)  # x1 from decoder path, x2 is skip connection from encoder\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)  # Concatenate skip connection (key U-Net feature!)\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "-YFEfnU7WmZF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetDown(L.LightningModule):\n",
        "  def __init__(self, n_channels=1, latent_dim=3, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.features = features\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, features[0])\n",
        "        self.downs = nn.ModuleList([Down(features[i], features[i+1]) for i in range(len(features)-1)])\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc_mu = nn.Linear(features[-1] * 4 * 4, latent_dim)  # example for 4x4 spatial size\n",
        "        self.fc_logvar = nn.Linear(features[-1] * 4 * 4, latent_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "        skip_connections = []\n",
        "        x = self.inc(x)\n",
        "        skip_connections.append(x)\n",
        "\n",
        "        for down in self.downs:  # Downsampling path\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        skip_connections = skip_connections[:-1]  # Remove last (bottleneck)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "\n",
        "        return x, mu, logvar, skip_connections\n",
        "\n",
        "\n",
        "class UNetUp(L.LightningModule):\n",
        "  def __init__(self, n_channels=1, latent_dim=3, n_classes=1, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.features = features\n",
        "\n",
        "        self.fc = nn.Linear(latent_dim, features[-1] * 4 * 4)  # mirror encoder\n",
        "        self.unflatten = nn.Unflatten(1, (features[-1], 4, 4))\n",
        "        self.ups = nn.ModuleList([Up(features[len(features)-1-i], features[len(features)-2-i]) for i in range(len(features)-1)])\n",
        "        self.outc = nn.Conv2d(features[0], n_classes, 1)\n",
        "\n",
        "  def forward(self, bottleneck, skip_connections):\n",
        "        x = self.fc(bottleneck)\n",
        "        x = self.unflatten(x)\n",
        "\n",
        "        for i, up in enumerate(self.ups):  # Upsampling path with skip connections\n",
        "            skip = skip_connections[-(i+1)]\n",
        "            x = up(x, skip)\n",
        "\n",
        "        return torch.sigmoid(self.outc(x))"
      ],
      "metadata": {
        "id": "USeDuCycWl_S"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instead of nn.Module, we use L.LightningModule\n",
        "class VAEModel(L.LightningModule):\n",
        "    def __init__(self,\n",
        "                 latent_dim=3,    # dimensionality of the latent space. bigger=less compression, better reconstruction\n",
        "                 features=[64,128,256,512],  # intermediate/hidden layers in our simple encoder/decoder\n",
        "                 act = nn.LeakyReLU,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        # VAE encoder outputs two values, for mean and variance\n",
        "        self.encoder = UNetDown(n_channels=1, latent_dim=latent_dim, features=features)\n",
        "\n",
        "        # VAE decoder is exactly same as vanilla AE\n",
        "        self.decoder = UNetUp(n_channels=1, n_classes=1, latent_dim=latent_dim, features=features)\n",
        "\n",
        "        self.use_conv = True\n",
        "        self.latent_dim, self.features, self.act = latent_dim, features, act # save config for archival purposes\n",
        "\n",
        "    def reparam_sample(self, mu, log_var):\n",
        "        \"this yields a data value by sampling from the learned gaussian distribution of latents\"\n",
        "        std = torch.exp(0.5*log_var).sqrt() # the 0.5 is an optional, tunable rescaling factor.\n",
        "        noise = torch.randn_like(std)       # the gaussian distribution we sample from\n",
        "        return mu + std * noise\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mu, log_var = self.encoder(x)\n",
        "        z_hat = self.reparam_sample(mu, log_var)\n",
        "        x_hat = self.decoder(z_hat)\n",
        "        return z, x_hat, mu, log_var, z_hat # order chosen to preserve earlier model's outputs of z, x_hat\n",
        "\n",
        "\n",
        "    def pred_and_log(self, batch, batch_idx, log_prefix=''):\n",
        "        \"the basic task: make predictions, compute loss, and send to logging system\"\n",
        "        x, y = batch\n",
        "        x = x.view(x.size(0), -1)  # flatten for linear layer\n",
        "        z, x_hat, mu, log_var, z_hat = self.forward(x)\n",
        "\n",
        "        #recon_loss = nn.functional.mse_loss(x_hat, x)\n",
        "        recon_loss = nn.functional.binary_cross_entropy(x_hat, x) # remember you need a sigmoid on decoder out to use bce\n",
        "        # or you could use torch.nn.functional.binary_cross_entropy_with_logits without the sigmoid\n",
        "        kl_loss = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        scrunch_factor = 1e-3 # larger means blurrier/blobbier\n",
        "        rescaled_kl_loss = (kl_loss * scrunch_factor) # this is what goes in the loss function\n",
        "        loss = recon_loss + rescaled_kl_loss\n",
        "\n",
        "        self.log(f'{log_prefix}loss', loss, prog_bar=True)\n",
        "        self.log(f'{log_prefix}recon_loss', recon_loss)\n",
        "        self.log(f'{log_prefix}kl_loss', kl_loss)\n",
        "        self.log(F'{log_prefix}rescaled_kl_loss', rescaled_kl_loss)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.pred_and_log(batch, batch_idx, log_prefix='train/')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.pred_and_log(batch, batch_idx, log_prefix='val/')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=5e-4)\n",
        "\n",
        "    def on_epoch_start(self): # give us a new line for each epoch\n",
        "        print('\\n')\n",
        "\n",
        "    def on_validation_epoch_end(self, demo_every=1):\n",
        "        if self.current_epoch % demo_every == 0:  # log every this many epochs\n",
        "            fig = test_inference(self, idx=range(5), return_fig=True)\n",
        "            self.logger.experiment.log({\"reconstructions\": wandb.Image(fig), \"epoch\": self.current_epoch})\n",
        "            plt.close(fig)  # clean up\n",
        "\n",
        "\n",
        "vae = VAEModel()\n",
        "model = vae"
      ],
      "metadata": {
        "id": "wwJZ93lr5xb1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()\n",
        "# Lightning defines its own WandbLogger callback...\n",
        "wandb.finish() # just in case we're aborting runs & restarting\n",
        "\n",
        "# set up the logging\n",
        "wandb_logger = WandbLogger(log_model=\"all\", project='vae_sim')\n",
        "try:\n",
        "    wandb_logger.watch(model)\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "P7rbL7Wi6Xxm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "3437a13c-e7a3-4491-e747-3ef4673be1e1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">smooth-planet-1</strong> at: <a href='https://wandb.ai/simeonbetapudi-belmont-university/vae_sim/runs/rwt4g6i0' target=\"_blank\">https://wandb.ai/simeonbetapudi-belmont-university/vae_sim/runs/rwt4g6i0</a><br> View project at: <a href='https://wandb.ai/simeonbetapudi-belmont-university/vae_sim' target=\"_blank\">https://wandb.ai/simeonbetapudi-belmont-university/vae_sim</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251006_172305-rwt4g6i0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20251006_172646-val5mw1f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/simeonbetapudi-belmont-university/vae_sim/runs/val5mw1f' target=\"_blank\">gentle-resonance-2</a></strong> to <a href='https://wandb.ai/simeonbetapudi-belmont-university/vae_sim' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/simeonbetapudi-belmont-university/vae_sim' target=\"_blank\">https://wandb.ai/simeonbetapudi-belmont-university/vae_sim</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/simeonbetapudi-belmont-university/vae_sim/runs/val5mw1f' target=\"_blank\">https://wandb.ai/simeonbetapudi-belmont-university/vae_sim/runs/val5mw1f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 25  # VAEs require more steps to train than vanilla AEs, due to dual-objective loss\n",
        "trainer = L.Trainer(max_epochs=epochs, devices=\"auto\", logger=wandb_logger, callbacks=RichProgressBar(leave=True))\n",
        "trainer.fit(model=vae, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "vtL3uxhtPknL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724,
          "referenced_widgets": [
            "eb0b0e2def3e4ce6a70f83183ad22caf",
            "7463dd51c6824946a607ec3f6524b266"
          ]
        },
        "outputId": "6ae9a32f-50b4-427f-a37d-9b21d7dbfcec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "INFO:lightning.pytorch.utilities.rank_zero:üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mType    \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m‚îÇ encoder ‚îÇ UNetDown ‚îÇ  4.7 M ‚îÇ train ‚îÇ\n",
              "‚îÇ\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m‚îÇ decoder ‚îÇ UNetUp   ‚îÇ  3.0 M ‚îÇ train ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type     </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>‚îÉ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>‚îÇ encoder ‚îÇ UNetDown ‚îÇ  4.7 M ‚îÇ train ‚îÇ\n",
              "‚îÇ<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>‚îÇ decoder ‚îÇ UNetUp   ‚îÇ  3.0 M ‚îÇ train ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 7.8 M                                                                                            \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 7.8 M                                                                                                \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 31                                                                         \n",
              "\u001b[1mModules in train mode\u001b[0m: 81                                                                                          \n",
              "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 7.8 M                                                                                            \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 7.8 M                                                                                                \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 31                                                                         \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 81                                                                                          \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb0b0e2def3e4ce6a70f83183ad22caf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 784]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3430090618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m  \u001b[0;31m# VAEs require more steps to train than vanilla AEs, due to dual-objective loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwandb_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRichProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         )\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m             \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         )\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1509926540.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_and_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1509926540.py\u001b[0m in \u001b[0;36mpred_and_log\u001b[0;34m(self, batch, batch_idx, log_prefix)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# flatten for linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#recon_loss = nn.functional.mse_loss(x_hat, x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1509926540.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mz_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparam_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1880\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1825\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1827\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1828\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m                 for hook_id, hook in (\n",
            "\u001b[0;32m/tmp/ipython-input-2979387528.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mskip_connections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mskip_connections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-811695143.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         )\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 784]"
          ]
        }
      ]
    }
  ]
}